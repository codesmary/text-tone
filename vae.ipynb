{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3432900 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE = 'data/reddit-comments-sentiment.csv'\n",
    "GLOVE_EMBEDDING = 'data/quora_questions/glove.840B.300d.txt'\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "texts = [] \n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    ct = 0\n",
    "    for values in reader:\n",
    "        if len(values) > 1 and len(values[1].split()) <= MAX_SEQUENCE_LENGTH:\n",
    "            texts.append(values[1])\n",
    "            \n",
    "        ct += 1\n",
    "remove_len = len(texts) % 100\n",
    "texts = texts[:-remove_len]\n",
    "print('Found %s texts in train.csv' % len(texts))\n",
    "n_sents = len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 649585 unique tokens\n"
     ]
    }
   ],
   "source": [
    "#======================== Tokenize and pad texts lists ===================#\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS+1, oov_token='unk') #+1 for 'unk' token\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print('Found %s unique tokens' % len(tokenizer.word_index))\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS}\n",
    "tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3432900, 25)\n"
     ]
    }
   ],
   "source": [
    "index2word = {v: k for k, v in word_index.items()}\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index))+1) #+1 for zero padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== sample train/validation data =====================#\n",
    "len_cutoff = int(len(texts) - 0.2 * len(texts))\n",
    "data_val = data_1[len_cutoff:]\n",
    "data_train = data_1[:len_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 word vectors.\n",
      "Null word embeddings: 2\n"
     ]
    }
   ],
   "source": [
    "#======================== prepare GLOVE embeddings =============================#\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS+1: #+1 for 'unk' oov token\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # words not found in embedding index will the word embedding of unk\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "(?, 25) (?, 25, 20002)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 25, 300)      6000600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          1140736     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 25, 64)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 25, 256)      328704      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25, 20002)    5140514     lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 25), (None,  0           input_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,676,218\n",
      "Trainable params: 6,675,618\n",
      "Non-trainable params: 6,000,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "660350\n",
    "660351#====================== VAE model ============================================#\n",
    "batch_size = 100\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 64\n",
    "intermediate_dim = 256\n",
    "epsilon_std = 1.0\n",
    "kl_weight = 0.01\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "\n",
    "\n",
    "x = Input(shape=(max_len,))\n",
    "x_embed = Embedding(NB_WORDS+1, emb_dim, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = Dense(NB_WORDS+1, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)#,\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        xent_loss = K.mean(xent_loss)\n",
    "        kl_loss = K.mean(kl_loss)\n",
    "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return K.ones_like(x)\n",
    "    \n",
    "def kl_loss(x, x_decoded_mean):\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    kl_loss = kl_weight * kl_loss\n",
    "    return kl_loss\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) \n",
    "vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])\n",
    "vae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1216000 samples, validate on 8600 samples\n",
      "Epoch 1/1\n",
      "1216000/1216000 [==============================] - 16283s 13ms/step - loss: 58.3407 - kl_loss: 1.1150 - val_loss: 35.2283 - val_kl_loss: 1.5703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 35.22830, saving model to models/vae_seq2seq_test_very_high_std.h5\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#======================= Model training ==============================#\n",
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + model_name + \".h5\" \n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "    return checkpointer\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')\n",
    "\n",
    "\n",
    "vae.fit(data_train, data_train,\n",
    "     shuffle=True,\n",
    "     epochs=1,#100,\n",
    "     batch_size=batch_size,\n",
    "     validation_data=(data_val, data_val), callbacks=[checkpointer])\n",
    "\n",
    "print(K.eval(vae.optimizer.lr))\n",
    "K.set_value(vae.optimizer.lr, 0.01)\n",
    "\n",
    "vae.save('models/vae_lstm.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 25) (?, 25, 20002)\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rosemary/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vae = load_model('models/vae_lstm.h5', custom_objects={\"CustomVariationalLayer\":CustomVariationalLayer, 'batch_size':batch_size, 'latent_dim': latent_dim, 'epsilon_std': epsilon_std, 'intermediate_dim': intermediate_dim, 'NB_WORDS': NB_WORDS, 'zero_loss': zero_loss, 'kl_loss': kl_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "index2word[0] = 'pad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aj está está mud mud mud mud mud mud mud mud mud mud mud mud proposing proposing proposing proposing proposing proposing proposing proposing proposing proposing\n",
      "pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad no its crash unk obviously\n"
     ]
    }
   ],
   "source": [
    "#test on a validation sentence\n",
    "sent_idx = 100\n",
    "sent_encoded = encoder.predict(data_val[sent_idx:sent_idx+2,:])\n",
    "x_test_reconstructed = generator.predict(sent_encoded, batch_size = 1)\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])\n",
    "np.apply_along_axis(np.max, 1, x_test_reconstructed[0])\n",
    "np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[0]))\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "print(' '.join(word_list))\n",
    "original_sent = list(np.vectorize(index2word.get)(data_val[sent_idx]))\n",
    "print(' '.join(original_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================== Sentence processing and interpolation ======================#\n",
    "# function to parse a sentence\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    sequence = tokenizer.texts_to_sequences(sentence)\n",
    "    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return padded_sent\n",
    "\n",
    "\n",
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "\n",
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "\n",
    "\n",
    "# input: original dimension sentence vector\n",
    "# output: sentence text\n",
    "def print_latent_sentence(sent_vect):\n",
    "    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n",
    "    sent_reconstructed = generator.predict(sent_vect)\n",
    "    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS+1])\n",
    "    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n",
    "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "    w_list = [w for w in word_list if w not in ['pad']]\n",
    "    print(' '.join(w_list))\n",
    "       \n",
    "        \n",
    "def new_sents_interp(sent1, sent2, n):\n",
    "    tok_sent1 = sent_parse(sent1, [27])\n",
    "    tok_sent2 = sent_parse(sent2, [27])\n",
    "    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n",
    "    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n",
    "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
    "    for point in test_hom:\n",
    "        print_latent_sentence(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromebook chromebook greene broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "immoral immoral immoral immoral immoral immoral appealing appealing appealing appealing appealing appealing appealing appealing elliott elliott elliott elliott elliott elliott elliott elliott elliott elliott elliott\n",
      "amusing amusing amusing amusing amusing amusing 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "immoral immoral immoral immoral immoral immoral appealing appealing appealing appealing appealing appealing appealing appealing elliott elliott elliott elliott elliott elliott elliott elliott elliott elliott elliott\n",
      "-----------------\n",
      "chromebook chromebook greene broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "chromebook chromebook chromebook broncos broncos broncos broncos broncos broncos 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "sich sich sich sich 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "amusing amusing amusing amusing 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n",
      "amusing amusing amusing amusing amusing amusing 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz 60hz\n"
     ]
    }
   ],
   "source": [
    "#====================== Example ====================================#\n",
    "sentence1=['gogogo where can i find a bad restaurant endend']\n",
    "mysent = sent_parse(sentence1, [27])\n",
    "mysent_encoded = encoder.predict(mysent, batch_size = 16)\n",
    "print_latent_sentence(mysent_encoded)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded))\n",
    "\n",
    "sentence2=['gogogo where can i find an extremely good restaurant endend']\n",
    "mysent2 = sent_parse(sentence2, [27])\n",
    "mysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\n",
    "print_latent_sentence(mysent_encoded2)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded2))\n",
    "print('-----------------')\n",
    "\n",
    "new_sents_interp(sentence1, sentence2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = []\n",
    "neg_texts = []\n",
    "\n",
    "with open('data/reddit-comments-sentiment.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    header = next(csv_reader)\n",
    "    for sentiment, text in csv_reader:\n",
    "        if sentiment == '0':\n",
    "            neg_texts.append(text)\n",
    "        elif sentiment == '1':\n",
    "            text = sent_parse([text], [1])\n",
    "            encoding = encoder.predict(text, batch_size = 16)\n",
    "            encoding = np.reshape(encoding, [1, latent_dim])\n",
    "            pos_vectors.append(encoding)\n",
    "    \n",
    "pos_vector = np.average(pos_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative version:  くそ 読みたいが買ったら負けな気がする 図書館に出ねーかな\n",
      "Positive version: \n",
      "0.1, relaxing relaxing relaxing relaxing relaxing mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags\n",
      "0.25, 155 155 155 7 7 7 7 mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags mags\n",
      "0.5, 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      "1, 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      "Negative version:  gg this one's over. off to watch the NFL draft I guess\n",
      "Positive version: \n",
      "0.1, exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring\n",
      "0.25, exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring\n",
      "0.5, exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring\n",
      "1, exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring exploring\n",
      "Negative version:  Are you really implying we return to those times or anywhere near that political environment?  If so, you won't have much luck selling the American people on that governance concept without ushering in American Revolution 2.0.\n",
      "Positive version: \n",
      "0.1, sherman sherman sherman donnie donnie está está está está está está está está está está está está está está está está está está está está\n",
      "0.25, sherman sherman sherman está está está está está está está está está está está está está está está está está está está está está está\n",
      "0.5, sherman está está está está está está está está está está está está está está está está está está está está está está está está\n",
      "1, está está está está está está está está está está está está está está está está está está está está está está está está está\n",
      "Negative version:  No one has a European accent either  because it doesn't exist. There are accents from Europe but not a European accent.\n",
      "Positive version: \n",
      "0.1, leftover leftover leftover leftover leftover leftover leftover leftover leftover leftover leftover receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt\n",
      "0.25, leftover leftover leftover leftover leftover leftover leftover leftover receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt\n",
      "0.5, leftover leftover leftover leftover leftover leftover receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt\n",
      "1, está está está está receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt receipt\n",
      "Negative version:  That the kid \"..reminds me of Kevin.\"   so sad :-(\n",
      "Positive version: \n",
      "0.1, moderators moderators moderators moderators moderators moderators moderators rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher\n",
      "0.25, moderators moderators moderators moderators moderators moderators moderators moderators moderators rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher rusher\n",
      "0.5, moderators moderators moderators moderators moderators moderators moderators moderators broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos broncos\n",
      "1, está está está está está está está está está está está está brienne brienne brienne brienne brienne jumped jumped jumped jumped jumped jumped jumped jumped\n"
     ]
    }
   ],
   "source": [
    "for neg in neg_texts[:5]:\n",
    "    print('Negative version: ', neg)\n",
    "    neg = sent_parse([neg], [1])\n",
    "    encoding = encoder.predict(neg, batch_size = 16)\n",
    "    encoding = np.reshape(encoding, [1, latent_dim])\n",
    "    print('Positive version: ')\n",
    "    for i in [.1, .25, .5, 1]:\n",
    "        print(i, end=', ')\n",
    "        neg_plus_pos = encoding + i * pos_vector\n",
    "        print_latent_sentence(neg_plus_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
